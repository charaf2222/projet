<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Opencv JS</title>
    <script async src="js/opencv.js" onload="openCvReady();"></script>
    <script src="js/utils.js"></script>
    <style>
        body, html {
            height: 100%;
            margin: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            background-color: #020202; /* Optional: Background color for the page */
        }
        #videoWrapper {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh; /* Adjusted to 100% of the viewport height */
        }
        #cam_input {
            display: none; /* Hide the video element */
        }
        #canvas_output {
            /* Additional styling to ensure canvas is centered if needed */
            /* You can specify width and height here if you want to fix the size of the canvas */
        }
    </style>
</head>
<body>
    <div id="videoWrapper">
        <video id="cam_input" height="480" width="640"></video>
        <canvas id="canvas_output"></canvas>
    </div>
<script type="text/JavaScript">
function openCvReady() {
    cv['onRuntimeInitialized']=()=>{
        let video = document.getElementById("cam_input"); // video is the id of video tag
        navigator.mediaDevices.getUserMedia({ video: true, audio: false })
        .then(function(stream) {
            video.srcObject = stream;
            video.play();
        })
        .catch(function(err) {
            console.log("An error occurred! " + err);
        });
        let src = new cv.Mat(video.height, video.width, cv.CV_8UC4);
        let dst = new cv.Mat(video.height, video.width, cv.CV_8UC1);
        let gray = new cv.Mat();
        let cap = new cv.VideoCapture(cam_input);
        let faces = new cv.RectVector();
        let classifier = new cv.CascadeClassifier();
        let utils = new Utils('errorMessage');
        let faceCascadeFile = 'haarcascade_frontalface_default.xml'; // path to xml
        utils.createFileFromUrl(faceCascadeFile, faceCascadeFile, () => {
            classifier.load(faceCascadeFile); // in the callback, load the cascade from file 
        });
        const FPS = 24;
        function processVideo() {
            let begin = Date.now();
            cap.read(src);
            src.copyTo(dst);
            cv.cvtColor(dst, gray, cv.COLOR_RGBA2GRAY, 0);
            try{
                classifier.detectMultiScale(gray, faces, 1.1, 3, 0);
                console.log(faces.size());
                // Affichage du visage extrait (pour vérification)
                console.log("C'est le visage extrait : __________________________");
                console.log("\n");
            }catch(err){
                console.log(err);
            }
            for (let i = 0; i < faces.size(); ++i) {
                let face = faces.get(i);
                let point1 = new cv.Point(face.x, face.y);
                let point2 = new cv.Point(face.x + face.width, face.y + face.height);
                cv.rectangle(dst, point1, point2, [0, 0, 255, 255]);
                // Extraction du visage
                let faceROI = src.roi(face); // Région d'intérêt (ROI) correspondant au visage
                // Assuming `faceROI` is your extracted face region as a cv.Mat object
                // Assuming `faceROI` is your extracted face region as a cv.Mat object from OpenCV.js
                // Assuming `faceROI` is your extracted face region as a cv.Mat object from OpenCV.js
                let rows = faceROI.rows;
                let cols = faceROI.cols;
                let structuredArray = [];

                for (let i = 0; i < rows; i++) {
                    let rowArray = [];
                    for (let j = 0; j < cols; j++) {
                        // Extracting only the RGB values, ignoring the Alpha channel
                        let pixel = [faceROI.ucharPtr(i, j)[0], // R
                                    faceROI.ucharPtr(i, j)[1], // G
                                    faceROI.ucharPtr(i, j)[2]]; // B
                        rowArray.push(pixel);
                    }
                    structuredArray.push(rowArray);
                }

                // Now `structuredArray` mimics the Python-like array without the Alpha channel.


                // To simulate the example output you provided (the length and a glimpse of the array structure)
                console.log("The length is:", structuredArray.length);
                console.log(JSON.stringify(structuredArray.slice(0, 3)) + "\n...\n"); // Show the beginning of the array for brevity

                // This console output is just to give you an idea. It won't exactly replicate Python's print output but gives a structured view of the data.

                fetch('http://127.0.0.1:8000/api/Etudient/', { // Assuming 'compare_faces' is your custom action
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        // Include any other headers your Django backend requires, like CSRF tokens for example
                    },
                    body: JSON.stringify({encodings: structuredArray}), // Make sure to stringify your payload
                })
                .then(response => {
                    if (!response.ok) {
                        throw new Error('Network response was not ok');
                    }
                    return response.json();
                })
                .then(data => console.log('Success:', data))
                .catch((error) => console.error('Error:', error));
                


            
            }
            cv.imshow("canvas_output", dst);
            // schedule next one.
            let delay = 1000/FPS - (Date.now() - begin);
            setTimeout(processVideo, delay);
        }
        // schedule first one.
        setTimeout(processVideo, 0);
    };
}
</script>
</html>
